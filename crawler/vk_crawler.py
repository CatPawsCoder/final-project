# —Ü–µ–ª—å -  –∏–º–µ—Ç—å –µ–¥–∏–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç:

# –ü–æ—Å—Ç—ã, –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —á–µ—Ä–µ–∑ newsfeed.search.

# –ü–æ—Å—Ç—ã –∏–∑ –∑–∞—Ä–∞–Ω–µ–µ –∑–∞–¥–∞–Ω–Ω—ã—Ö –¥–µ–ø—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –≥—Ä—É–ø–ø (–ø–æ ID).


import vk_api
from config import VK_TOKEN
import pandas as pd
import time
from datetime import datetime, timedelta
import os
import csv

# –°–ª–æ–≤–∞, –ø–æ –∫–æ—Ç–æ—Ä—ã–º —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ—Å—Ç—ã (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å)
depression_keywords = [
    '–¥–µ–ø—Ä–µ—Å—Å–∏—è', '—É—Å—Ç–∞–ª', '—Ç–æ—Å–∫–∞', '–±–æ–ª—å', '–æ–¥–∏–Ω–æ—á–µ—Å—Ç–≤–æ', '–Ω–µ —Ö–æ—á—É –∂–∏—Ç—å', '–Ω–∏—á–µ–≥–æ –Ω–µ —Ä–∞–¥—É–µ—Ç',
    '–ø—É—Å—Ç–æ—Ç–∞', '–±–µ–∑—ã—Å—Ö–æ–¥–Ω–æ—Å—Ç—å', '–ø–æ–¥–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç—å', '–Ω–µ –≤–∏–∂—É —Å–º—ã—Å–ª–∞', '—Ö–∞–Ω–¥—Ä–∞', '—Ç—Ä–µ–≤–æ–≥–∞', '—Å—Ç—Ä–∞—Ö',
    '—Å–ª–µ–∑—ã', '–∞–ø–∞—Ç–∏—è', '–ø—Å–∏—Ö–æ—Ç–µ—Ä–∞–ø–∏—è', '–≤—ã–≥–æ—Ä–∞–Ω–∏–µ', '–æ—Ç—á–∞—è–Ω–∏–µ'
]

def contains_keywords(text):
    text_lower = text.lower()
    return any(kw in text_lower for kw in depression_keywords)

def to_unix(dt):
    return int(dt.timestamp())

def get_posts_by_search(query, start_time, end_time, count=200):
    vk_session = vk_api.VkApi(token=VK_TOKEN)
    vk = vk_session.get_api()

    try:
        response = vk.newsfeed.search(q=query, count=count, start_time=start_time, end_time=end_time)
        return response.get('items', [])
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ VK API –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ—Å—Ç–æ–≤: {e}")
        return []

def load_group_ids(input_file="data/depression_groups.csv"):
    group_ids = []
    try:
        with open(input_file, mode='r', encoding='utf-8') as file:
            reader = csv.reader(file)
            next(reader)  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            group_ids = [row[0] for row in reader]
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ ID –≥—Ä—É–ø–ø: {e}")
    return group_ids

def crawl_by_keywords_and_groups(output_csv="data/raw/vk_depression_dataset_full_search.csv", days_back=90, step_days=7):
    today = datetime.now()
    intervals = [
        (today - timedelta(days=i + step_days), today - timedelta(days=i))
        for i in range(0, days_back, step_days)
    ]

    collected = []

    if os.path.exists(output_csv):
        existing_df = pd.read_csv(output_csv)
        existing_texts = set(existing_df['text'])
    else:
        existing_df = None
        existing_texts = set()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–µ–ø—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –≥—Ä—É–ø–ø—ã
    depression_groups = load_group_ids(input_file="data/depression_groups.csv")
    print(f"–ù–∞–π–¥–µ–Ω–æ {len(depression_groups)} –¥–µ–ø—Ä–µ—Å—Å–∏–≤–Ω—ã—Ö –≥—Ä—É–ø–ø.")

    # –°–æ–±–∏—Ä–∞–µ–º –ø–æ—Å—Ç—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    for keyword in depression_keywords:
        print(f"üîç –ö–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ: {keyword}")
        for start, end in intervals:
            unix_start = to_unix(start)
            unix_end = to_unix(end)

            print(f"  ‚è≥ –ü–µ—Ä–∏–æ–¥: {start.date()} ‚Äî {end.date()}")
            items = get_posts_by_search(keyword, unix_start, unix_end)

            for item in items:
                text = item.get('text', '').strip()
                if not text or text in existing_texts:
                    continue
                if not contains_keywords(text):
                    continue

                post_data = {
                    'text': text,
                    'date': item.get('date'),
                    'from_id': item.get('from_id'),
                    'post_id': item.get('id'),
                    'likes': item.get('likes', {}).get('count', 0),
                    'comments': item.get('comments', {}).get('count', 0)
                }
                collected.append(post_data)
                existing_texts.add(text)

            time.sleep(1.2)  # –ø–∞—É–∑–∞ —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –±–∞–Ω–∞

    # –°–æ–±–∏—Ä–∞–µ–º –ø–æ—Å—Ç—ã –∏–∑ –≥—Ä—É–ø–ø
    for group_id in depression_groups:
        print(f"üîç –°–æ–±–∏—Ä–∞–µ–º –ø–æ—Å—Ç—ã –∏–∑ –≥—Ä—É–ø–ø—ã: {group_id}")
        for start, end in intervals:
            unix_start = to_unix(start)
            unix_end = to_unix(end)

            print(f"  ‚è≥ –ü–µ—Ä–∏–æ–¥: {start.date()} ‚Äî {end.date()}")
            try:
                response = vk_api.VkApi(token=VK_TOKEN).get_api().wall.get(owner_id=-int(group_id), count=200, offset=0)
                items = response.get('items', [])
                for item in items:
                    text = item.get('text', '').strip()
                    if not text or text in existing_texts:
                        continue
                    if not contains_keywords(text):
                        continue

                    post_data = {
                        'text': text,
                        'date': item.get('date'),
                        'from_id': item.get('from_id'),
                        'post_id': item.get('id'),
                        'likes': item.get('likes', {}).get('count', 0),
                        'comments': item.get('comments', {}).get('count', 0)
                    }
                    collected.append(post_data)
                    existing_texts.add(text)

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–µ –ø–æ—Å—Ç–æ–≤ –∏–∑ –≥—Ä—É–ø–ø—ã {group_id}: {e}")
                continue

            time.sleep(1.2)  # –ø–∞—É–∑–∞ —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –±–∞–Ω–∞

    if collected:
        new_df = pd.DataFrame(collected)
        if existing_df is not None:
            full_df = pd.concat([existing_df, new_df], ignore_index=True).drop_duplicates(subset='text')
        else:
            full_df = new_df

        os.makedirs(os.path.dirname(output_csv), exist_ok=True)
        full_df.to_csv(output_csv, index=False)
        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(collected)} –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤. –í—Å–µ–≥–æ: {len(full_df)}")
    else:
        print("‚ùå –ù–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

# –ü—Ä–∏–º–µ—Ä –≤—ã–∑–æ–≤–∞ —Ñ—É–Ω–∫—Ü–∏–∏
if __name__ == "__main__":
    crawl_by_keywords_and_groups()







# import vk_api
# from config import VK_TOKEN
# import pandas as pd
# import time


# def get_wall_posts(group_id, total_posts=1000):
#     vk_session = vk_api.VkApi(token=VK_TOKEN)
#     vk = vk_session.get_api()

#     posts = []
#     offset = 0
#     batch_size = 100

#     while len(posts) < total_posts:
#         try:
#             response = vk.wall.get(owner_id=-group_id, count=batch_size, offset=offset)
#             items = response.get('items', [])
#             if not items:
#                 break
#             for item in items:
#                 if 'text' in item:
#                     text = item['text'].strip()
#                     if text and not any(p['text'] == text for p in posts):
#                         # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ñ–æ—Ç–æ
#                         image_url = ''
#                         if 'attachments' in item:
#                             for att in item['attachments']:
#                                 if att['type'] == 'photo':
#                                     sizes = att['photo'].get('sizes', [])
#                                     if sizes:
#                                         image_url = sizes[-1].get('url')

#                         post_data = {
#                             'source': f"group_{group_id}",
#                             'text': text,
#                             'date': item.get('date'),
#                             'from_id': item.get('from_id'),
#                             'post_id': item.get('id'),
#                             'likes': item.get('likes', {}).get('count', 0),
#                             'comments': item.get('comments', {}).get('count', 0),
#                             'image_url': image_url
#                         }
#                         posts.append(post_data)
#             offset += batch_size
#             time.sleep(1.5)
#         except Exception as e:
#             print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ: {e}")
#             break

#     return posts

# if __name__ == "__main__":
#     # ID –≥—Ä—É–ø–ø (—á–∏—Å–ª–æ–≤—ã–µ), –º–æ–∂–Ω–æ –±—Ä–∞—Ç—å –Ω–∞ —Å–∞–π—Ç–µ vk.com/groups –∏–ª–∏ —á–µ—Ä–µ–∑ vk.com/id...
#     group_ids = [
#         203728426,  # psychologytoday
#         55873153,   # psylive
#         213673161,  # vyhoranie
#         192618568,  # samopomosh
#         132863911   # anti-depressant
#     ]

#     all_data = []
#     collected_texts = set()

#     for group_id in group_ids:
#         print(f"–°–æ–±–∏—Ä–∞–µ–º –ø–æ—Å—Ç—ã –∏–∑ –≥—Ä—É–ø–ø—ã: {group_id}")
#         posts = get_wall_posts(group_id, total_posts=500)
#         print(f"–ù–∞–π–¥–µ–Ω–æ {len(posts)} –ø–æ—Å—Ç–æ–≤ –∏–∑ –≥—Ä—É–ø–ø—ã {group_id}")
#         for post in posts:
#             if post['text'] not in collected_texts:
#                 all_data.append(post)
#                 collected_texts.add(post['text'])

#     # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π CSV, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
#     try:
#         old_df = pd.read_csv('data/raw/vk_depression_dataset_full_with_images.csv')
#         new_df = pd.DataFrame(all_data)
#         combined_df = pd.concat([old_df, new_df], ignore_index=True)
#         # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏
#         combined_df = combined_df.drop_duplicates(subset='text')
#         combined_df.to_csv('data/raw/vk_depression_dataset_full_with_images.csv', index=False)
#         print(f"–ù–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã. –ò—Ç–æ–≥–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤: {len(combined_df)}")
#     except FileNotFoundError:
#         # –ï—Å–ª–∏ CSV –µ—â—ë –Ω–µ—Ç ‚Äî —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π
#         new_df = pd.DataFrame(all_data)
#         new_df.to_csv('data/raw/vk_depression_dataset_full_with_images.csv', index=False)
#         print(f"–°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π CSV. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤: {len(new_df)}")


# #  –≤–µ—Ä—Å–∏—è 1 —Å–±–æ—Ä –ø–æ—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ –º–µ—Ç–æ–¥ ‚Äî newsfeed.search –∏ –ø–æ–∏—Å–∫ –ø–æ —Å–ª–æ–≤–∞–º.
# # def get_vk_posts(query, total_posts=500):
# #     vk_session = vk_api.VkApi(token=VK_TOKEN)
# #     vk = vk_session.get_api()

# #     posts = []
# #     offset = 0
# #     batch_size = 200

# #     while len(posts) < total_posts:
# #         try:
# #             response = vk.newsfeed.search(q=query, count=batch_size, offset=offset)
# #             items = response.get('items', [])
# #             if not items:
# #                 break
# #             for item in items:
# #                 if 'text' in item and not any(p['text'] == item['text'] for p in posts):
# #                     # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ñ–æ—Ç–æ
# #                     image_url = ''
# #                     if 'attachments' in item:
# #                         for att in item['attachments']:
# #                             if att['type'] == 'photo':
# #                                 sizes = att['photo'].get('sizes', [])
# #                                 if sizes:
# #                                     image_url = sizes[-1].get('url')  # –°–∞–º–æ–µ –±–æ–ª—å—à–æ–µ —Ñ–æ—Ç–æ

# #                     post_data = {
# #                         'keyword': query,
# #                         'text': item['text'],
# #                         'date': item.get('date'),
# #                         'from_id': item.get('from_id'),
# #                         'post_id': item.get('id'),
# #                         'owner_id': item.get('owner_id'),
# #                         'likes': item.get('likes', {}).get('count', 0),
# #                         'comments': item.get('comments', {}).get('count', 0),
# #                         'image_url': image_url
# #                     }
# #                     posts.append(post_data)
# #             offset += batch_size
# #             time.sleep(0.5)
# #         except Exception as e:
# #             print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ: {e}")
# #             break

# #     return posts

# # if __name__ == "__main__":
# #     keywords = [
# #         '–¥–µ–ø—Ä–µ—Å—Å–∏—è',
# #         '–≥—Ä—É—Å—Ç—å',
# #         '—Ç–æ—Å–∫–∞',
# #         '–ø–µ—á–∞–ª—å',
# #         '—É—Å—Ç–∞–ª–æ—Å—Ç—å',
# #         '–Ω–µ —Ö–æ—á—É –∂–∏—Ç—å',
# #         '–Ω–∏—á–µ–≥–æ –Ω–µ —Ä–∞–¥—É–µ—Ç',
# #         '–æ–¥–∏–Ω–æ—á–µ—Å—Ç–≤–æ',
# #         '–ø–∞–Ω–∏–∫–∞',
# #         '—Ç—Ä–µ–≤–æ–≥–∞',
# #         '–ø—É—Å—Ç–æ—Ç–∞',
# #         '–±–µ–∑—ã—Å—Ö–æ–¥–Ω–æ—Å—Ç—å',
# #         '–±–æ–ª—å',
# #         '—Ö–æ—á—É —É–º–µ—Ä–µ—Ç—å',
# #         '–æ—Ç—á–∞—è–Ω–∏–µ',
# #         '–ø–æ–¥–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç—å',
# #         '–Ω–µ –≤–∏–∂—É —Å–º—ã—Å–ª–∞',
# #         '—Ö–∞–Ω–¥—Ä–∞',
# #         '—Å–ª—ë–∑—ã',
# #         '—Å—Ç—Ä–∞—Ö',
# #         '–∞–ø–∞—Ç–∏—è'
# #     ]

# #     all_data = []

# #     for keyword in keywords:
# #         print(f"–°–æ–±–∏—Ä–∞–µ–º –ø–æ—Å—Ç—ã –ø–æ —Å–ª–æ–≤—É: {keyword}")
# #         posts = get_vk_posts(keyword, total_posts=500)
# #         all_data.extend(posts)

# #     df = pd.DataFrame(all_data)
# #     df.to_csv('data/raw/vk_depression_dataset_full_with_images.csv', index=False)
# #     print("–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à—ë–Ω. –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ data/raw/vk_depression_dataset_full_with_images.csv")
